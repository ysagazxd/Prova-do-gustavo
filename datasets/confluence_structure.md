# ğŸ“‹ ESTRUTURA CONFLUENCE - Sistema de AnÃ¡lise de Vendas E-commerce

## ğŸ—ï¸ HIERARQUIA DE PÃGINAS

```
ğŸ“ Sistema de AnÃ¡lise de Vendas E-commerce (PÃGINA PRINCIPAL)
â”œâ”€â”€ ğŸ“„ 1. VisÃ£o Geral do Projeto
â”œâ”€â”€ ğŸ“„ 2. Arquitetura do Sistema  
â”œâ”€â”€ ğŸ“„ 3. Guia de ExecuÃ§Ã£o
â”œâ”€â”€ ğŸ“„ 4. DocumentaÃ§Ã£o TÃ©cnica Completa
â”œâ”€â”€ ğŸ“„ 5. AnÃ¡lise de Dados
â””â”€â”€ ğŸ“„ 6. Resumo Executivo
```

---

## ğŸ“„ PÃGINA PRINCIPAL - Sistema de AnÃ¡lise de Vendas E-commerce

### ConteÃºdo da PÃ¡gina Principal:

**TÃ­tulo:** Sistema de AnÃ¡lise de Vendas E-commerce

**DescriÃ§Ã£o:** Pipeline completo de dados para anÃ¡lise de performance de vendas

### VisÃ£o Geral
Este projeto implementa um pipeline completo de dados para anÃ¡lise de vendas de e-commerce, desde a ingestÃ£o atÃ© a visualizaÃ§Ã£o de insights de negÃ³cio.

### Problema Abordado
AnÃ¡lise de performance de vendas, identificaÃ§Ã£o de produtos mais vendidos, sazonalidade e comportamento de clientes para tomada de decisÃ£o estratÃ©gica.

### Arquitetura
```
Dados CSV â†’ IngestÃ£o (Python) â†’ Processamento (Spark) â†’ Data Lake (MinIO) â†’ VisualizaÃ§Ã£o (Metabase)
```

### Como Executar
1. **PrÃ©-requisitos:** Docker e Docker Compose, Python 3.9+
2. **Executar:** `docker-compose up -d` â†’ `python src/pipeline.py`
3. **Acessar:** MinIO (http://localhost:9001), Metabase (http://localhost:3000)

### Estrutura do Projeto
```
â”œâ”€â”€ docs/           # DocumentaÃ§Ã£o completa
â”œâ”€â”€ src/            # CÃ³digo-fonte do pipeline
â”œâ”€â”€ infra/          # Docker Compose e configs
â”œâ”€â”€ notebooks/      # AnÃ¡lise exploratÃ³ria
â”œâ”€â”€ datasets/       # Dados de exemplo
â””â”€â”€ README.md       # Este arquivo
```

### Equipe
- **Gustavo**: Arquitetura geral e processamento de dados
- **[Colega]**: VisualizaÃ§Ã£o e anÃ¡lise de dados

### Tecnologias Utilizadas
- **IngestÃ£o**: Python, Pandas
- **Processamento**: Apache Spark
- **Armazenamento**: MinIO (S3-compatible)
- **VisualizaÃ§Ã£o**: Metabase
- **OrquestraÃ§Ã£o**: Docker Compose

### PÃ¡ginas Filhas
- [VisÃ£o Geral do Projeto]
- [Arquitetura do Sistema]
- [Guia de ExecuÃ§Ã£o]
- [DocumentaÃ§Ã£o TÃ©cnica Completa]
- [AnÃ¡lise de Dados]
- [Resumo Executivo]

---

## ğŸ“„ PÃGINA 1 - VisÃ£o Geral do Projeto

### DescriÃ§Ã£o do Problema

#### Contexto de NegÃ³cio
O projeto aborda a necessidade de uma empresa de e-commerce analisar suas vendas para:
- Identificar produtos e categorias mais rentÃ¡veis
- Entender comportamento sazonal das vendas
- Segmentar clientes por valor e comportamento
- Otimizar estratÃ©gias de marketing e estoque

#### Desafios TÃ©cnicos
- Volume crescente de dados de vendas
- Necessidade de processamento em batch e potencial streaming
- MÃºltiplas fontes de dados (vendas, produtos, clientes)
- Demanda por dashboards em tempo real
- Escalabilidade para crescimento futuro

### Objetivos do Sistema

#### Objetivos Principais
- **CentralizaÃ§Ã£o**: Unificar dados de vendas em Data Lake
- **Processamento**: Pipeline automatizado de ETL
- **AnÃ¡lise**: Gerar insights de negÃ³cio atravÃ©s de KPIs
- **VisualizaÃ§Ã£o**: Dashboards interativos para tomada de decisÃ£o

#### Objetivos TÃ©cnicos
- Implementar arquitetura de Data Lake (Bronze/Silver/Gold)
- Processar dados com Apache Spark para escalabilidade
- Armazenar dados em formato otimizado (Parquet)
- Disponibilizar dados via PostgreSQL para BI
- Criar dashboards no Metabase

### Escopo da SoluÃ§Ã£o

#### IncluÃ­do no Escopo
- Pipeline de ingestÃ£o de dados CSV
- Processamento batch com Spark
- Data Lake com 4 camadas (Raw/Bronze/Silver/Gold)
- Armazenamento em MinIO (S3-compatible)
- Banco PostgreSQL para consultas OLAP
- Dashboards no Metabase
- AnÃ¡lise exploratÃ³ria em Jupyter
- DocumentaÃ§Ã£o completa
- Scripts de automaÃ§Ã£o

#### NÃ£o IncluÃ­do no Escopo
- IngestÃ£o em tempo real (streaming)
- APIs REST para consulta de dados
- AutenticaÃ§Ã£o e autorizaÃ§Ã£o avanÃ§ada
- Monitoramento e alertas automatizados
- Backup e disaster recovery automatizado
- IntegraÃ§Ã£o com sistemas externos (CRM, ERP)

---

## ğŸ“„ PÃGINA 2 - Arquitetura do Sistema

### VisÃ£o Geral da Arquitetura

#### Diagrama de Componentes
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Dados CSV     â”‚â”€â”€â”€â–¶â”‚   IngestÃ£o       â”‚â”€â”€â”€â–¶â”‚   Processamento â”‚â”€â”€â”€â–¶â”‚   Armazenamento  â”‚
â”‚   (Fonte)       â”‚    â”‚   (Python)       â”‚    â”‚   (Spark)       â”‚    â”‚   (MinIO S3)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚
                                                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Dashboards    â”‚â—€â”€â”€â”€â”‚   PostgreSQL     â”‚â—€â”€â”€â”€â”‚   ExportaÃ§Ã£o    â”‚
â”‚   (Metabase)    â”‚    â”‚   (OLAP)         â”‚    â”‚   (Pipeline)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Camadas da Arquitetura

#### Camada de IngestÃ£o
- **Tecnologia**: Python + Pandas
- **FunÃ§Ã£o**: Coleta e validaÃ§Ã£o inicial dos dados
- **Formato de Entrada**: CSV
- **ValidaÃ§Ãµes**: VerificaÃ§Ã£o de integridade, tipos de dados

#### Camada de Processamento
- **Tecnologia**: Apache Spark
- **FunÃ§Ã£o**: TransformaÃ§Ãµes, limpeza e agregaÃ§Ãµes
- **ParalelizaÃ§Ã£o**: DistribuÃ­do para grandes volumes
- **OtimizaÃ§Ãµes**: Cache de DataFrames, particionamento

#### Camada de Armazenamento (Data Lake)

**Raw Layer (Dados Brutos)**
- **Bucket**: `raw-data`
- **Formato**: Parquet (compressÃ£o eficiente)
- **ConteÃºdo**: Dados originais sem transformaÃ§Ã£o
- **RetenÃ§Ã£o**: Permanente para auditoria

**Bronze Layer (Dados Limpos)**
- **Bucket**: `bronze-data`
- **Formato**: Parquet particionado por data
- **TransformaÃ§Ãµes**: RemoÃ§Ã£o de registros invÃ¡lidos, padronizaÃ§Ã£o de tipos

**Silver Layer (Dados Agregados)**
- **Bucket**: `silver-data`
- **Formato**: Parquet otimizado
- **ConteÃºdo**: AgregaÃ§Ãµes por categoria/perÃ­odo, mÃ©tricas de clientes

**Gold Layer (Dados para Consumo)**
- **Bucket**: `gold-data`
- **Formato**: Parquet + PostgreSQL
- **ConteÃºdo**: KPIs principais, dados prontos para dashboards

#### Camada de ApresentaÃ§Ã£o
- **Tecnologia**: Metabase
- **FunÃ§Ã£o**: Dashboards e relatÃ³rios
- **ConexÃ£o**: PostgreSQL como fonte
- **AtualizaÃ§Ãµes**: Batch diÃ¡rio via pipeline

### Fluxo de Dados

1. **IngestÃ£o**: CSV â†’ ValidaÃ§Ã£o â†’ Raw Layer
2. **Bronze**: Raw â†’ Limpeza â†’ Bronze Layer
3. **Silver**: Bronze â†’ AgregaÃ§Ãµes â†’ Silver Layer
4. **Gold**: Silver â†’ KPIs â†’ Gold Layer + PostgreSQL
5. **VisualizaÃ§Ã£o**: PostgreSQL â†’ Metabase â†’ Dashboards

---

## ğŸ“„ PÃGINA 3 - Guia de ExecuÃ§Ã£o

### PrÃ©-requisitos

#### Software NecessÃ¡rio
- **Docker Desktop**: VersÃ£o 4.0+
- **Docker Compose**: VersÃ£o 2.0+
- **Python**: VersÃ£o 3.9+
- **Git**: Para versionamento

#### Recursos MÃ­nimos
- **RAM**: 8GB (recomendado 16GB)
- **Storage**: 10GB livres
- **CPU**: 4 cores (recomendado)

### ExecuÃ§Ã£o Passo a Passo

#### 1. Subir a Infraestrutura
```bash
cd infra
docker-compose up -d
docker-compose ps
```

#### 2. Gerar Dados de Exemplo
```bash
cd ..
python src/generate_data.py
```

#### 3. Executar Pipeline de Dados
```bash
python src/pipeline.py
```

### VerificaÃ§Ã£o dos ServiÃ§os

#### MinIO (Data Lake)
- **URL**: http://localhost:9001
- **UsuÃ¡rio**: admin
- **Senha**: password123

#### PostgreSQL
```bash
docker exec -it postgres psql -U postgres -d sales_db
\dt
SELECT COUNT(*) FROM sales_summary;
```

#### Metabase
- **URL**: http://localhost:3000
- **ConfiguraÃ§Ã£o**: PostgreSQL (host: postgres, porta: 5432, db: sales_db)

### Troubleshooting

#### Containers nÃ£o sobem
```bash
netstat -an | findstr "3000 5432 8080 9000"
docker stop $(docker ps -q)
docker system prune -f
```

#### Erro no pipeline Spark
```bash
docker logs spark-master
docker logs spark-worker
docker-compose restart spark-master spark-worker
```

---

## ğŸ“„ PÃGINA 4 - DocumentaÃ§Ã£o TÃ©cnica Completa

### Tecnologias e Ferramentas

#### Stack TecnolÃ³gico

| Componente | Tecnologia | VersÃ£o | Justificativa |
|------------|------------|--------|---------------|
| Processamento | Apache Spark | 3.4 | Escalabilidade, performance |
| Storage | MinIO | Latest | S3-compatible, open-source |
| Database | PostgreSQL | 13 | OLAP, performance analÃ­tica |
| BI | Metabase | Latest | Open-source, fÃ¡cil uso |
| Linguagem | Python | 3.9+ | Ecossistema data science |
| OrquestraÃ§Ã£o | Docker Compose | 2.0+ | Portabilidade, isolamento |

#### DecisÃµes TÃ©cnicas

**Por que Spark?**
- **PrÃ³s**: Escalabilidade, performance, ecossistema
- **Contras**: Complexidade, overhead para pequenos dados
- **Alternativas**: Pandas (limitado), Dask (menos maduro)
- **DecisÃ£o**: Spark pela escalabilidade futura

**Por que MinIO?**
- **PrÃ³s**: S3-compatible, open-source, performance
- **Contras**: Menos features que AWS S3
- **Alternativas**: HDFS (complexo), filesystem local (nÃ£o escalÃ¡vel)
- **DecisÃ£o**: MinIO pela compatibilidade e simplicidade

### Dados e Schema

#### Schema dos Dados
```sql
order_id VARCHAR(20) -- Identificador Ãºnico do pedido
customer_id INTEGER -- ID do cliente
customer_segment VARCHAR(20) -- Segmento (Premium/Regular/BÃ¡sico)
product_name VARCHAR(100) -- Nome do produto
category VARCHAR(50) -- Categoria do produto
price DECIMAL(10,2) -- PreÃ§o unitÃ¡rio
quantity INTEGER -- Quantidade vendida
total_amount DECIMAL(12,2) -- Valor total (price * quantity)
sale_date DATE -- Data da venda
rating DECIMAL(3,1) -- AvaliaÃ§Ã£o do produto (1-5)
```

### Pontos de Falha e LimitaÃ§Ãµes

#### LimitaÃ§Ãµes Atuais
- **Escalabilidade**: Single-node, nÃ£o cluster
- **Performance**: Sem otimizaÃ§Ãµes avanÃ§adas
- **Disponibilidade**: Sem redundÃ¢ncia ou failover
- **Monitoramento**: Manual, sem alertas automÃ¡ticos
- **Dados**: SintÃ©ticos, nÃ£o refletem realidade
- **Tempo real**: Apenas batch, sem streaming

#### MitigaÃ§Ãµes Implementadas
- **ValidaÃ§Ã£o de dados**: Filtros de qualidade
- **Logs detalhados**: Para troubleshooting
- **ContainerizaÃ§Ã£o**: Isolamento e portabilidade
- **DocumentaÃ§Ã£o**: Guias de execuÃ§Ã£o e troubleshooting

### Trabalho Individual

#### Gustavo (VocÃª)
- **Arquitetura geral**: Design do pipeline e componentes
- **Processamento de dados**: ImplementaÃ§Ã£o Spark, transformaÃ§Ãµes
- **Data Lake**: Estrutura de camadas, formatos de dados
- **Infraestrutura**: Docker Compose, configuraÃ§Ãµes
- **DocumentaÃ§Ã£o**: Arquitetura, guias tÃ©cnicos

#### [Seu Colega]
- **AnÃ¡lise de dados**: Jupyter notebooks, estatÃ­sticas
- **VisualizaÃ§Ã£o**: Dashboards Metabase, KPIs
- **ValidaÃ§Ã£o**: Testes de qualidade, validaÃ§Ã£o de resultados
- **DocumentaÃ§Ã£o**: AnÃ¡lises, insights de negÃ³cio

---

## ğŸ“„ PÃGINA 5 - AnÃ¡lise de Dados

### Dados Gerados

#### EstatÃ­sticas Gerais
- **Volume**: 10.000 registros de vendas
- **PerÃ­odo**: 2023-2024
- **Valor total**: R$ 16.7 milhÃµes
- **Categorias**: 6 (EletrÃ´nicos, Roupas, Casa, Livros, Esportes, Beleza)
- **Clientes**: 1.986 Ãºnicos

### KPIs Principais

#### Receita por Categoria
- **EletrÃ´nicos**: ~R$ 5M (lÃ­der)
- **Roupas**: ~R$ 3M
- **Casa**: ~R$ 2.5M
- **Outros**: ~R$ 6.2M

#### SegmentaÃ§Ã£o de Clientes
- **Premium**: Maior valor mÃ©dio por pedido
- **Regular**: Volume mÃ©dio
- **BÃ¡sico**: Maior quantidade de transaÃ§Ãµes

### AnÃ¡lise ExploratÃ³ria

#### Jupyter Notebook
```bash
jupyter notebook notebooks/exploratory_analysis.ipynb
```

#### Principais Insights
- Sazonalidade identificada nos dados
- PadrÃµes de comportamento por segmento
- Top produtos por receita e quantidade
- CorrelaÃ§Ãµes entre rating e vendas

### VisualizaÃ§Ãµes

#### Metabase Dashboards
- Dashboard de vendas por categoria
- AnÃ¡lise temporal de receita
- Performance de produtos
- MÃ©tricas de clientes

---

## ğŸ“„ PÃGINA 6 - Resumo Executivo

### O que foi entregue

#### Estrutura Completa
- Pipeline completo: CSV â†’ Python â†’ Spark â†’ MinIO â†’ PostgreSQL â†’ Metabase
- Data Lake: 4 camadas (Raw/Bronze/Silver/Gold)
- Processamento: Apache Spark distribuÃ­do
- Armazenamento: MinIO (S3-compatible) + PostgreSQL
- VisualizaÃ§Ã£o: Metabase + Jupyter notebooks
- Infraestrutura: Docker Compose (5 containers)

### Como Executar (3 passos)

1. **Setup AutomÃ¡tico**: `setup.bat`
2. **Executar Pipeline**: `python src/pipeline.py`
3. **Acessar Resultados**: MinIO (localhost:9001), Metabase (localhost:3000)

### Resultados DemonstrÃ¡veis

#### KPIs Principais
- Receita por categoria: EletrÃ´nicos lidera com ~R$ 5M
- Clientes Premium: Maior valor mÃ©dio por pedido
- Sazonalidade: PadrÃµes mensais identificados
- Top produtos: Ranking por receita e quantidade

#### MÃ©tricas TÃ©cnicas
- Performance: Pipeline completo em ~2 minutos
- Escalabilidade: Spark distribuÃ­do, storage S3-compatible
- Qualidade: 100% dos dados validados e processados
- Formato: Parquet otimizado (compressÃ£o ~70%)

### Diferenciais do Projeto

#### Pontos Fortes
- **Funciona 100%**: Pipeline executÃ¡vel do zero
- **DocumentaÃ§Ã£o completa**: Guias detalhados, troubleshooting
- **Tecnologias modernas**: Spark, containers, Data Lake
- **Boas prÃ¡ticas**: Camadas, validaÃ§Ãµes, versionamento
- **EscalÃ¡vel**: Design pensado para produÃ§Ã£o

#### Conhecimento Demonstrado
- **Big Data**: Spark, processamento distribuÃ­do
- **Engenharia de Dados**: ETL, Data Lake, formatos otimizados
- **DevOps**: Docker, orquestraÃ§Ã£o, infraestrutura como cÃ³digo
- **AnÃ¡lise**: Jupyter, visualizaÃ§Ãµes, insights

### Melhorias Futuras

#### Curto Prazo (1-3 meses)
- Agendamento: Airflow para orquestraÃ§Ã£o
- Monitoramento: Prometheus + Grafana
- Testes: UnitÃ¡rios e integraÃ§Ã£o
- CI/CD: Pipeline automatizado

#### MÃ©dio Prazo (3-6 meses)
- Streaming: Kafka + Spark Streaming
- APIs: REST endpoints para dados
- ML: Modelos preditivos (churn, recomendaÃ§Ã£o)
- SeguranÃ§a: AutenticaÃ§Ã£o, criptografia

#### Longo Prazo (6+ meses)
- Cloud: MigraÃ§Ã£o para AWS/Azure/GCP
- Cluster: Spark/Hadoop distribuÃ­do
- Data Mesh: Arquitetura descentralizada
- Real-time: Dashboards em tempo real

---

## ğŸ“‹ INSTRUÃ‡Ã•ES PARA CONFLUENCE

### Como Criar no Confluence:

1. **Criar EspaÃ§o**: "Sistema de AnÃ¡lise de Vendas E-commerce"
2. **PÃ¡gina Principal**: Copiar conteÃºdo da "PÃGINA PRINCIPAL"
3. **PÃ¡ginas Filhas**: Criar 6 pÃ¡ginas filhas com os conteÃºdos respectivos
4. **Links**: Ajustar links internos para pÃ¡ginas do Confluence
5. **FormataÃ§Ã£o**: Usar macros do Confluence para cÃ³digo e diagramas

### Macros Ãšteis do Confluence:
- **{code}** para blocos de cÃ³digo
- **{info}** para caixas de informaÃ§Ã£o
- **{warning}** para alertas
- **{toc}** para Ã­ndice automÃ¡tico
- **{children}** para listar pÃ¡ginas filhas

### Anexos Recomendados:
- Screenshots dos dashboards
- Diagramas de arquitetura
- Logs de execuÃ§Ã£o
- CÃ³digo-fonte principal (pipeline.py)