# Guia de Execu√ß√£o e Depend√™ncias

## üõ†Ô∏è **REQUISITOS OBRIGAT√ìRIOS PARA RODAR O PROJETO**

### **‚úÖ DEVE TER INSTALADO:**
- **Docker** (vers√£o 20.10.0 ou superior)
- **Docker Compose** (vers√£o 2.0.0 ou superior) 
- **Python** (vers√£o 3.9.0 ou superior)

### **‚ö†Ô∏è OPCIONAL (para pipeline Spark completo):**
- **Java 17+** (se n√£o tiver, use pipeline simplificado)

### **üîç VERIFICAR SE TEM:**
```bash
docker --version          # Precisa: 20.10.0+
docker-compose --version  # Precisa: 2.0.0+
python --version          # Precisa: 3.9.0+
java -version             # Opcional: 17+ (para Spark)
```

### **üì¶ INSTALA√á√ÉO AUTOM√ÅTICA:**
```bash
# Execute apenas este comando (instala tudo automaticamente):
setup.bat
```

### **üì¶ INSTALA√á√ÉO MANUAL:**
```bash
# 1. Instalar depend√™ncias Python
pip install pandas numpy boto3 psycopg2-binary matplotlib seaborn python-dotenv

# 2. Subir containers Docker
cd infra && docker-compose up -d

# 3. Executar pipeline
python src/pipeline_simple.py
```

---

## üöÄ Guia de Execu√ß√£o: Como Rodar do Zero

### Pr√©-requisitos Detalhados

#### Software Necess√°rio
```bash
# Verificar se est√° instalado
docker --version          # M√≠nimo: 20.10.0
docker-compose --version  # M√≠nimo: 2.0.0
python --version          # M√≠nimo: 3.9.0
git --version             # Qualquer vers√£o recente
```

#### Recursos de Hardware
- **RAM**: 8GB m√≠nimo (16GB recomendado)
- **CPU**: 4 cores m√≠nimo (8 cores recomendado)
- **Storage**: 10GB livres (SSD prefer√≠vel)
- **Rede**: Conex√£o para download de imagens Docker

#### Portas Necess√°rias
```bash
# Verificar se as portas est√£o livres
netstat -an | findstr "3000 5432 7077 8080 9000 9001"
# Se alguma porta estiver ocupada, parar o servi√ßo correspondente
```

### M√©todo 1: Execu√ß√£o Autom√°tica (Recomendado)

#### Passo √önico
```bash
# Executar script de setup completo
setup.bat
```

**O que o script faz:**
1. Verifica pr√©-requisitos
2. Instala depend√™ncias Python
3. Sobe infraestrutura Docker
4. Aguarda inicializa√ß√£o dos servi√ßos
5. Executa pipeline de dados
6. Valida resultados

### M√©todo 2: Execu√ß√£o Manual Passo a Passo

#### Passo 1: Prepara√ß√£o do Ambiente
```bash
# 1. Clonar reposit√≥rio (se necess√°rio)
git clone https://github.com/ysagazxd/Prova-do-gustavo.git
cd Prova-do-gustavo

# 2. Criar ambiente virtual Python
python -m venv venv

# 3. Ativar ambiente virtual
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate

# 4. Instalar depend√™ncias
pip install -r requirements.txt
```

#### Passo 2: Configura√ß√£o de Vari√°veis (Opcional)
```bash
# Criar arquivo .env para customiza√ß√µes
echo MINIO_ROOT_USER=admin > .env
echo MINIO_ROOT_PASSWORD=password123 >> .env
echo POSTGRES_PASSWORD=postgres123 >> .env
echo SPARK_WORKER_MEMORY=2g >> .env
echo SPARK_WORKER_CORES=2 >> .env
```

#### Passo 3: Subir Infraestrutura
```bash
# Navegar para diret√≥rio de infraestrutura
cd infra

# Subir todos os servi√ßos em background
docker-compose up -d

# Verificar se todos os containers subiram
docker-compose ps
```

**Sa√≠da Esperada:**
```
NAME                IMAGE                     STATUS
minio               minio/minio:latest        Up
metabase            metabase/metabase:latest  Up
postgres            postgres:13               Up
spark-master        bitnami/spark:3.4         Up
spark-worker        bitnami/spark:3.4         Up
```

#### Passo 4: Aguardar Inicializa√ß√£o
```bash
# Aguardar 2-3 minutos para todos os servi√ßos iniciarem
# Verificar logs se necess√°rio
docker-compose logs -f metabase

# Testar conectividade
curl -I http://localhost:9001  # MinIO Console
curl -I http://localhost:8080  # Spark UI
curl -I http://localhost:3000  # Metabase
```

#### Passo 5: Gerar Dados de Exemplo
```bash
# Voltar para diret√≥rio raiz
cd ..

# Executar gera√ß√£o de dados sint√©ticos
python src/generate_data.py
```

**Sa√≠da Esperada:**
```
Gerando dados sint√©ticos de vendas...
Dados gerados: 10000 registros
Per√≠odo: 2023-01-01 a 2024-12-31
Categorias: 6 (Eletr√¥nicos, Roupas, Casa & Jardim, Livros, Esportes, Beleza)
Clientes √∫nicos: 2000
Produtos √∫nicos: 30
Total de vendas: R$ 15,234,567.89
Arquivo salvo: datasets/sales_data.csv
```

#### Passo 6: Executar Pipeline Principal
```bash
# Executar pipeline completo de ETL
python src/pipeline.py
```

**Sa√≠da Esperada:**
```
=== Sistema de An√°lise de Vendas E-commerce ===
=== Iniciando Pipeline de Dados ===

[INFO] Configurando conex√µes...
[INFO] MinIO conectado: http://localhost:9000
[INFO] Spark Session iniciada: SalesAnalytics
[INFO] PostgreSQL conectado: sales_db

[INFO] Criando buckets no Data Lake...
‚úì Bucket 'raw-data' criado
‚úì Bucket 'bronze-data' criado  
‚úì Bucket 'silver-data' criado
‚úì Bucket 'gold-data' criado

[INFO] Processando camada RAW...
‚úì Dados carregados: 10000 registros
‚úì Valida√ß√µes executadas: 100% v√°lidos
‚úì Dados salvos: raw-data/sales_data_20241201.parquet

[INFO] Processando camada BRONZE...
‚úì Filtros de qualidade aplicados
‚úì Padroniza√ß√£o de tipos conclu√≠da
‚úì Dados salvos: bronze-data/year=2024/month=12/

[INFO] Processando camada SILVER...
‚úì Agrega√ß√µes por categoria calculadas
‚úì M√©tricas de clientes processadas
‚úì Dados salvos: silver-data/

[INFO] Processando camada GOLD...
‚úì KPIs principais calculados
‚úì Top produtos identificados
‚úì Dados salvos: gold-data/

[INFO] Exportando para PostgreSQL...
‚úì Tabela sales_summary: 10000 registros
‚úì Tabela customer_metrics: 2000 registros
‚úì Tabela product_performance: 30 registros

=== Pipeline executado com sucesso! ===
Tempo total: 2m 34s
```

### Verifica√ß√£o de Resultados

#### Acessar Interfaces Web
```bash
# MinIO Console (Data Lake)
# URL: http://localhost:9001
# User: admin / Pass: password123

# Spark UI (Monitoramento)
# URL: http://localhost:8080

# Metabase (Dashboards)
# URL: http://localhost:3000
# Configurar conex√£o PostgreSQL na primeira vez
```

#### Verificar Dados no PostgreSQL
```bash
# Conectar via linha de comando
docker exec -it postgres psql -U postgres -d sales_db

# Verificar tabelas criadas
\dt

# Contar registros
SELECT 'sales_summary' as tabela, COUNT(*) as registros FROM sales_summary
UNION ALL
SELECT 'customer_metrics' as tabela, COUNT(*) as registros FROM customer_metrics
UNION ALL  
SELECT 'product_performance' as tabela, COUNT(*) as registros FROM product_performance;

# Sair do PostgreSQL
\q
```

#### Executar An√°lise Explorat√≥ria
```bash
# Iniciar Jupyter Notebook
jupyter notebook notebooks/

# Abrir e executar: exploratory_analysis.ipynb
# Verificar gr√°ficos e estat√≠sticas geradas
```

---

## üì¶ Guia Completo de Depend√™ncias

### Depend√™ncias Python (requirements.txt)

#### Core Data Processing
```txt
# Manipula√ß√£o de dados
pandas==2.1.4
numpy==1.24.3
pyspark==3.4.1

# Conectividade
boto3==1.34.0              # Cliente S3/MinIO
psycopg2-binary==2.9.9     # Driver PostgreSQL
sqlalchemy==2.0.23         # ORM database

# Visualiza√ß√£o
matplotlib==3.7.2
seaborn==0.12.2
plotly==5.17.0

# An√°lise interativa
jupyter==1.0.0
ipykernel==6.25.0
notebook==7.0.6

# Utilit√°rios
python-dotenv==1.0.0       # Vari√°veis de ambiente
faker==20.1.0              # Dados sint√©ticos
tqdm==4.66.1               # Progress bars
loguru==0.7.2              # Logging avan√ßado
```

#### Instala√ß√£o das Depend√™ncias
```bash
# M√©todo 1: Via requirements.txt
pip install -r requirements.txt

# M√©todo 2: Instala√ß√£o individual
pip install pandas==2.1.4 numpy==1.24.3 pyspark==3.4.1
pip install boto3==1.34.0 psycopg2-binary==2.9.9
pip install matplotlib==3.7.2 seaborn==0.12.2
pip install jupyter==1.0.0 faker==20.1.0
```

### Imagens Docker Utilizadas

#### Servi√ßos Principais
```yaml
# MinIO - Object Storage
minio/minio:latest
# Tamanho: ~50MB
# Fun√ß√£o: Data Lake S3-compatible

# Apache Spark - Processamento
bitnami/spark:3.4
# Tamanho: ~800MB  
# Fun√ß√£o: Master e Worker nodes

# PostgreSQL - Database OLAP
postgres:13
# Tamanho: ~150MB
# Fun√ß√£o: Armazenamento estruturado

# Metabase - Business Intelligence
metabase/metabase:latest
# Tamanho: ~300MB
# Fun√ß√£o: Dashboards e visualiza√ß√£o
```

#### Download das Imagens
```bash
# Download manual (opcional)
docker pull minio/minio:latest
docker pull bitnami/spark:3.4
docker pull postgres:13
docker pull metabase/metabase:latest

# Verificar imagens baixadas
docker images
```

### Configura√ß√µes de Sistema

#### Docker Compose (infra/docker-compose.yml)
```yaml
version: '3.8'

services:
  minio:
    image: minio/minio:latest
    container_name: minio
    ports:
      - "9000:9000"    # API
      - "9001:9001"    # Console
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: password123
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data
    networks:
      - data_network

  spark-master:
    image: bitnami/spark:3.4
    container_name: spark-master
    ports:
      - "8080:8080"    # Web UI
      - "7077:7077"    # Master port
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
    networks:
      - data_network

  spark-worker:
    image: bitnami/spark:3.4
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
    depends_on:
      - spark-master
    networks:
      - data_network

  postgres:
    image: postgres:13
    container_name: postgres
    environment:
      POSTGRES_DB: sales_db
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres123
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    networks:
      - data_network

  metabase:
    image: metabase/metabase:latest
    container_name: metabase
    ports:
      - "3000:3000"
    environment:
      MB_DB_TYPE: postgres
      MB_DB_DBNAME: metabase
      MB_DB_PORT: 5432
      MB_DB_USER: postgres
      MB_DB_PASS: postgres123
      MB_DB_HOST: postgres
    depends_on:
      - postgres
    volumes:
      - metabase_data:/metabase-data
    networks:
      - data_network

volumes:
  minio_data:
  postgres_data:
  metabase_data:

networks:
  data_network:
    driver: bridge
```

#### Configura√ß√£o PostgreSQL (infra/init.sql)
```sql
-- Criar database para dados de vendas
CREATE DATABASE sales_db;

-- Conectar ao database
\c sales_db;

-- Criar extens√µes √∫teis
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pg_stat_statements";

-- Configura√ß√µes de performance
ALTER SYSTEM SET shared_buffers = '256MB';
ALTER SYSTEM SET effective_cache_size = '1GB';
ALTER SYSTEM SET maintenance_work_mem = '64MB';
ALTER SYSTEM SET checkpoint_completion_target = 0.9;
ALTER SYSTEM SET wal_buffers = '16MB';
ALTER SYSTEM SET default_statistics_target = 100;
ALTER SYSTEM SET random_page_cost = 1.1;

-- Recarregar configura√ß√µes
SELECT pg_reload_conf();
```

### Vers√µes e Compatibilidade

#### Matriz de Compatibilidade
| Componente | Vers√£o Testada | Vers√£o M√≠nima | Vers√£o M√°xima |
|------------|----------------|---------------|---------------|
| Python | 3.9.18 | 3.9.0 | 3.11.x |
| Docker | 24.0.7 | 20.10.0 | Latest |
| Docker Compose | 2.23.3 | 2.0.0 | Latest |
| Spark | 3.4.1 | 3.3.0 | 3.5.x |
| PostgreSQL | 13.13 | 13.0 | 15.x |
| MinIO | RELEASE.2024-01-16 | 2023.x | Latest |

#### Verifica√ß√£o de Vers√µes
```bash
# Script de verifica√ß√£o completa
python --version
docker --version
docker-compose --version
pip list | grep -E "(pandas|pyspark|boto3)"

# Verificar containers em execu√ß√£o
docker ps --format "table {{.Names}}\t{{.Image}}\t{{.Status}}"
```