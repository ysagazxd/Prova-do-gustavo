# Arquitetura Completa do Pipeline

## üèóÔ∏è Vis√£o Geral da Arquitetura

### Diagrama de Componentes
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Dados CSV     ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Ingest√£o       ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Processamento ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Armazenamento  ‚îÇ
‚îÇ   (Fonte)       ‚îÇ    ‚îÇ   (Python)       ‚îÇ    ‚îÇ   (Spark)       ‚îÇ    ‚îÇ   (MinIO S3)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                         ‚îÇ
                                                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Dashboards    ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ   PostgreSQL     ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ   Exporta√ß√£o    ‚îÇ
‚îÇ   (Metabase)    ‚îÇ    ‚îÇ   (OLAP)         ‚îÇ    ‚îÇ   (Pipeline)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Fluxo de Dados Detalhado
```
[CSV Files] 
    ‚Üì (Python Pandas)
[Validation & Cleaning]
    ‚Üì (Spark ETL)
[Raw Layer - MinIO]
    ‚Üì (Spark Transformations)
[Bronze Layer - MinIO]
    ‚Üì (Spark Aggregations)
[Silver Layer - MinIO]
    ‚Üì (Spark KPIs)
[Gold Layer - MinIO]
    ‚Üì (JDBC Export)
[PostgreSQL OLAP]
    ‚Üì (SQL Queries)
[Metabase Dashboards]
```

---

## üîÑ Camadas do Pipeline

### 1. Camada de Ingest√£o

#### Componente: Python + Pandas
**Responsabilidades:**
- **Leitura de arquivos CSV** com encoding autom√°tico
- **Valida√ß√£o inicial** de schema e tipos de dados
- **Limpeza b√°sica** de dados inconsistentes
- **Convers√£o para Parquet** para otimiza√ß√£o de storage

**Valida√ß√µes Implementadas:**
- **Campos obrigat√≥rios**: Verifica√ß√£o de valores nulos
- **Tipos de dados**: Convers√£o e valida√ß√£o autom√°tica
- **Ranges v√°lidos**: Pre√ßos > 0, quantidades > 0, ratings 1-5
- **Consist√™ncia**: total_amount = price √ó quantity

**C√≥digo de Exemplo:**
```python
def validate_sales_data(df):
    # Validar campos obrigat√≥rios
    required_fields = ['order_id', 'customer_id', 'price', 'quantity']
    for field in required_fields:
        if df[field].isnull().any():
            raise ValueError(f"Campo {field} tem valores nulos")
    
    # Validar ranges
    if (df['price'] <= 0).any():
        raise ValueError("Pre√ßos devem ser positivos")
    
    # Validar consist√™ncia
    calculated_total = df['price'] * df['quantity']
    if not np.allclose(df['total_amount'], calculated_total):
        raise ValueError("Inconsist√™ncia no c√°lculo de total_amount")
```

### 2. Camada de Processamento

#### Componente: Apache Spark
**Configura√ß√£o:**
- **Spark Master**: Coordenador de jobs e recursos
- **Spark Worker**: Executor de tarefas distribu√≠das
- **Memory**: 1GB por worker configurado
- **Cores**: 1 core por worker para paraleliza√ß√£o

**Otimiza√ß√µes Implementadas:**
- **Cache de DataFrames**: Para reutiliza√ß√£o em m√∫ltiplas opera√ß√µes
- **Broadcast Joins**: Para tabelas pequenas (lookup tables)
- **Particionamento**: Por data para otimizar consultas temporais
- **Catalyst Optimizer**: Otimiza√ß√£o autom√°tica de queries

**Transforma√ß√µes por Camada:**

#### Raw ‚Üí Bronze
```python
def process_bronze_layer(spark, raw_path):
    df = spark.read.parquet(raw_path)
    
    # Filtros de qualidade
    df_clean = df.filter(
        (col("price") > 0) & 
        (col("quantity") > 0) & 
        (col("rating").between(1, 5))
    )
    
    # Padroniza√ß√£o de tipos
    df_clean = df_clean.withColumn("sale_date", to_date(col("sale_date")))
    df_clean = df_clean.withColumn("processed_at", current_timestamp())
    
    return df_clean
```

#### Bronze ‚Üí Silver
```python
def process_silver_layer(spark, bronze_path):
    df = spark.read.parquet(bronze_path)
    
    # Agrega√ß√µes por categoria
    category_metrics = df.groupBy("category") \
        .agg(
            sum("total_amount").alias("total_revenue"),
            count("order_id").alias("total_orders"),
            avg("rating").alias("avg_rating"),
            sum("quantity").alias("units_sold")
        )
    
    # M√©tricas de clientes
    customer_metrics = df.groupBy("customer_id", "customer_segment") \
        .agg(
            sum("total_amount").alias("customer_ltv"),
            count("order_id").alias("total_orders"),
            max("sale_date").alias("last_purchase")
        )
    
    return category_metrics, customer_metrics
```

#### Silver ‚Üí Gold
```python
def process_gold_layer(spark, silver_path):
    # KPIs principais para dashboards
    df = spark.read.parquet(silver_path)
    
    # Top produtos por receita
    top_products = df.groupBy("product_name") \
        .agg(sum("total_amount").alias("revenue")) \
        .orderBy(desc("revenue")) \
        .limit(20)
    
    # Vendas por m√™s
    monthly_sales = df.groupBy(
        date_trunc("month", col("sale_date")).alias("month")
    ).agg(
        sum("total_amount").alias("monthly_revenue"),
        countDistinct("customer_id").alias("unique_customers")
    )
    
    return top_products, monthly_sales
```

### 3. Camada de Armazenamento

#### Componente: MinIO (S3-Compatible)
**Configura√ß√£o:**
- **Endpoint**: http://localhost:9000 (API)
- **Console**: http://localhost:9001 (Web UI)
- **Credenciais**: admin/password123
- **Buckets**: raw-data, bronze-data, silver-data, gold-data

**Estrutura de Buckets:**
```
minio/
‚îú‚îÄ‚îÄ raw-data/
‚îÇ   ‚îî‚îÄ‚îÄ sales_data_20241201.parquet
‚îú‚îÄ‚îÄ bronze-data/
‚îÇ   ‚îî‚îÄ‚îÄ year=2024/month=12/day=01/
‚îÇ       ‚îî‚îÄ‚îÄ part-00000.parquet
‚îú‚îÄ‚îÄ silver-data/
‚îÇ   ‚îú‚îÄ‚îÄ category_metrics/
‚îÇ   ‚îî‚îÄ‚îÄ customer_metrics/
‚îî‚îÄ‚îÄ gold-data/
    ‚îú‚îÄ‚îÄ top_products/
    ‚îî‚îÄ‚îÄ monthly_sales/
```

**Otimiza√ß√µes de Storage:**
- **Formato Parquet**: Compress√£o Snappy (~70% redu√ß√£o)
- **Particionamento**: Por data para consultas eficientes
- **Schema Evolution**: Suporte a mudan√ßas de schema
- **Versionamento**: Controle de vers√µes autom√°tico

### 4. Camada OLAP

#### Componente: PostgreSQL
**Configura√ß√£o:**
- **Vers√£o**: PostgreSQL 13
- **Database**: sales_db
- **Usu√°rio**: postgres/postgres123
- **Porta**: 5432

**Schema Otimizado:**
```sql
-- Tabela principal de vendas
CREATE TABLE sales_summary (
    order_id VARCHAR(20) PRIMARY KEY,
    customer_id INTEGER,
    customer_segment VARCHAR(20),
    product_name VARCHAR(100),
    category VARCHAR(50),
    price DECIMAL(10,2),
    quantity INTEGER,
    total_amount DECIMAL(12,2),
    sale_date DATE,
    rating DECIMAL(3,1)
);

-- √çndices para performance
CREATE INDEX idx_sales_date ON sales_summary(sale_date);
CREATE INDEX idx_sales_category ON sales_summary(category);
CREATE INDEX idx_sales_customer ON sales_summary(customer_id);
CREATE INDEX idx_sales_segment ON sales_summary(customer_segment);

-- Tabela de m√©tricas de clientes
CREATE TABLE customer_metrics (
    customer_id INTEGER PRIMARY KEY,
    customer_segment VARCHAR(20),
    total_orders INTEGER,
    total_revenue DECIMAL(12,2),
    avg_order_value DECIMAL(10,2),
    last_purchase_date DATE
);

-- Tabela de performance de produtos
CREATE TABLE product_performance (
    product_name VARCHAR(100) PRIMARY KEY,
    category VARCHAR(50),
    total_revenue DECIMAL(12,2),
    total_orders INTEGER,
    avg_rating DECIMAL(3,1),
    units_sold INTEGER
);
```

### 5. Camada de Visualiza√ß√£o

#### Componente: Metabase
**Configura√ß√£o:**
- **URL**: http://localhost:3000
- **Database**: PostgreSQL (sales_db)
- **Conex√£o**: Autom√°tica via Docker network

**Dashboards Implementados:**
1. **Vis√£o Executiva**: KPIs principais e tend√™ncias
2. **An√°lise de Produtos**: Performance e rankings
3. **Segmenta√ß√£o de Clientes**: Comportamento e valor
4. **An√°lise Temporal**: Sazonalidade e crescimento

---

## üîß Infraestrutura e Orquestra√ß√£o

### Docker Compose
**Arquivo de Configura√ß√£o:**
```yaml
version: '3.8'
services:
  minio:
    image: minio/minio:latest
    ports: ["9000:9000", "9001:9001"]
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: password123
    command: server /data --console-address ":9001"
    volumes: [minio_data:/data]

  spark-master:
    image: apache/spark:3.4.0
    container_name: spark-master
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    environment:
      - SPARK_MASTER_HOST=0.0.0.0
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
    ports:
      - "8080:8080"
      - "7077:7077"

  spark-worker:
    image: apache/spark:3.4.0
    container_name: spark-worker
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1g

  postgres:
    image: postgres:13
    environment:
      POSTGRES_DB: sales_db
      POSTGRES_PASSWORD: postgres123
    ports: ["5432:5432"]
    volumes: [postgres_data:/var/lib/postgresql/data]

  metabase:
    image: metabase/metabase:latest
    container_name: metabase
    ports:
      - "3000:3000"
    environment:
      MB_DB_TYPE: h2
      MB_DB_FILE: /metabase-data/metabase.db
    volumes:
      - metabase_data:/metabase-data
    depends_on:
      - postgres
```

### Rede e Comunica√ß√£o
- **Rede Docker**: Isolamento e comunica√ß√£o segura
- **Service Discovery**: Resolu√ß√£o autom√°tica de nomes
- **Health Checks**: Verifica√ß√£o de sa√∫de dos servi√ßos
- **Volumes Persistentes**: Dados mantidos entre restarts

### Monitoramento
- **Logs Centralizados**: Docker logs para todos os servi√ßos
- **M√©tricas de Performance**: Tempo de execu√ß√£o do pipeline
- **Spark UI**: Monitoramento de jobs e recursos
- **MinIO Console**: Monitoramento de storage e objetos